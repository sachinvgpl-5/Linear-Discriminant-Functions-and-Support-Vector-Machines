{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: Linear Discriminant Functions and Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Multi-class support vector classifier with dot-product kernel and 1-norm soft margin using the MNIST training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist import MNIST\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn import model_selection, metrics, svm\n",
    "import matplotlib.pyplot as plt\n",
    "mnist = MNIST('./dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load training and testing data from the dataset\n",
    "x_train, y_train = mnist.load_training()\n",
    "x_test, y_test = mnist.load_testing()\n",
    "\n",
    "#assign training images to X_train and training labels to y_train\n",
    "x_train = np.asarray(x_train).astype(np.float32)/255.0\n",
    "y_train = np.asarray(y_train).astype(np.int32)\n",
    "\n",
    "#assign testing images to X_test and testing labels to y_test\n",
    "x_test = np.asarray(x_test).astype(np.float32)/255.0\n",
    "y_test = np.asarray(y_test).astype(np.int32)\n",
    "\n",
    "X_train = x_train.reshape(-1,784)\n",
    "Y_train = y_train.reshape(X_train.shape[0],)\n",
    "X_test = x_test.reshape(-1,784)\n",
    "Y_test = y_test.reshape(X_test.shape[0],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, C is a regularization parameter that controls the trade-off between maximizing the margin and minimizing the training error. Small C tends to emphasize the margin while ignoring the outliers in the training data, while large C may tend to overfit the training data. We calculate the accuracy of the Support Vector Classifier using k-fold Cross-Validation by combining the test data into the training data and taking the value of K=7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9140288411646743"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using a C value of 1\n",
    "SVM = svm.LinearSVC(C=1, penalty='l1', dual=False)\n",
    "\n",
    "cv_score = model_selection.cross_validate(SVM, X=np.concatenate((X_train, X_test)), y=np.concatenate((Y_train, Y_test)), cv=7)\n",
    "\n",
    "accuracy_C_1 = np.mean(cv_score['test_score'])\n",
    "accuracy_C_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9135859925632452"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using a C value of 2\n",
    "SVM = svm.LinearSVC(C=2, penalty='l1', dual=False)\n",
    "\n",
    "cv_score = model_selection.cross_validate(SVM, X=np.concatenate((X_train, X_test)), y=np.concatenate((Y_train, Y_test)), cv=7)\n",
    "accuracy_C_2 = np.mean(cv_score['test_score'])\n",
    "accuracy_C_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9129574625268236"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using a C value of 5\n",
    "SVM = svm.LinearSVC(C=5, penalty='l1', dual=False)\n",
    "\n",
    "cv_score = model_selection.cross_validate(SVM, X=np.concatenate((X_train, X_test)), y=np.concatenate((Y_train, Y_test)), cv=7)\n",
    "accuracy_C_5 = np.mean(cv_score['test_score'])\n",
    "accuracy_C_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9138574083251046"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using a C value of 0.1\n",
    "SVM = svm.LinearSVC(C=0.1, penalty='l1', dual=False)\n",
    "\n",
    "cv_score = model_selection.cross_validate(SVM, X=np.concatenate((X_train, X_test)), y=np.concatenate((Y_train, Y_test)), cv=7)\n",
    "accuracy_C_0_1 = np.mean(cv_score['test_score'])\n",
    "accuracy_C_0_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9141860025839611"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using a C value of 0.5\n",
    "SVM = svm.LinearSVC(C=0.5, penalty='l1', dual=False)\n",
    "cv_score = model_selection.cross_validate(SVM, X=np.concatenate((X_train, X_test)), y=np.concatenate((Y_train, Y_test)), cv=7)\n",
    "accuracy_C_0_5 = np.mean(cv_score['test_score'])\n",
    "accuracy_C_0_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9142860183029609"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using a C value of 0.6\n",
    "SVM = svm.LinearSVC(C=0.6, penalty='l1', dual=False)\n",
    "\n",
    "cv_score = model_selection.cross_validate(SVM, X=np.concatenate((X_train, X_test)), y=np.concatenate((Y_train, Y_test)), cv=7)\n",
    "accuracy_C_0_6 = np.mean(cv_score['test_score'])\n",
    "accuracy_C_0_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9141431368748157"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using a C value of 0.7\n",
    "SVM = svm.LinearSVC(C=0.7, penalty='l1', dual=False)\n",
    "\n",
    "cv_score = model_selection.cross_validate(SVM, X=np.concatenate((X_train, X_test)), y=np.concatenate((Y_train, Y_test)), cv=7)\n",
    "accuracy_C_0_7 = np.mean(cv_score['test_score'])\n",
    "accuracy_C_0_7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above accuracy values determined using Cross Validation, the value of hyperparameter C=0.6 gives the best accuracy of 91.42%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Lagrange Primal and Dual Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given features $(x_{1}, y_{1}), ... ,(x_{N}, y_{N})$, where    $y_{1},..,y_{N} \\epsilon$ {-1,1}\n",
    "\n",
    "The hard margin for Primal problem is\n",
    "\n",
    "Minimize $w^{T}.w + C\\sum_{i=1}^{N}\\xi_{i}$ where w is the separating vector, $w_{T}.w$ is the dot product, and $\\xi_{i}$ is the error made by the separating vector w on feature $(x_{i}, y_{i})$\n",
    "\n",
    "Subject to $y_{i}.(w^{T}.x_{i})\\geq 1 - \\xi_{i}$ and $\\xi_{i}\\geq0$ for i = 1,..,N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : we explicitly add the bias to the constraint, rather than treating it as a part of $x_{i}$, to aide in calculation.\n",
    "\n",
    "The corresponding Lagrangian for the 1-norm soft margin optimization problem is,\n",
    "\n",
    "$L(w, b, \\xi, \\alpha, r) = \\frac{1}{2}w^{T}.w + C\\sum_{i=1}^{N}\\xi_{i} - \\sum_{i=1}^{N}\\alpha_{i}[y_{i}(w^{T}.x_{i}+b)-1+\\xi_{i}] - \\sum_{i=1}^{N}r_{i}\\xi_{i}$\n",
    "\n",
    "\n",
    "where $\\alpha_{i}$ and $r_{i}$  are Lagrangian multipliers and \n",
    "$\\alpha_{i}\\geq0$ and $r_{i}\\geq0 $\n",
    "\n",
    "\n",
    "Differentiating the Legrangian with respect to w, $\\xi$ and b, we have\n",
    "\n",
    "$\\frac{\\partial L(w,b,\\xi,\\alpha,r)}{\\partial w} = w - \\sum_{i=1}^{N}y_{i}\\alpha_{i}x_{i}=0$\n",
    "\n",
    "$w = \\sum_{i=1}^{N}y_{i}\\alpha_{i}x_{i}$\n",
    "\n",
    "$\\frac{\\partial L(w,b,\\xi,\\alpha,r)}{\\partial \\xi_{i}} = C-\\alpha_{i} - r_{i} = 0$\n",
    "\n",
    "$ C = \\alpha_{i} + r_{i}$\n",
    "\n",
    "\n",
    "$\\frac{\\partial L(w,b,\\xi,\\alpha,r)}{\\partial b} = \\sum_{i=1}^{N}y_{i}\\alpha_{i} = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resubstituting these relations into the primal, we have the following dual problem:\n",
    "\n",
    "\n",
    "maximize $ L_{d}(\\alpha, r) = \\sum_{i=1}^{N}\\alpha_{i} - \\frac{1}{2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}y_{i}y_{j}\\alpha_{i}\\alpha_{j}(x_{j}^{T}.x_{i}) - \\sum_{i=1}^{N}\\alpha_{i}\\xi_{i} - \\sum_{i=1}^{N}r_{i}\\xi_{i} - C\\sum_{i=1}^{N}\\xi_{i}$\n",
    "\n",
    "    \n",
    " $= \\sum_{i=1}^{N}\\alpha_{i}- \\frac{1}{2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}y_{i}y_{j}\\alpha_{i}\\alpha_{j}(x_{j}^{T}.x_{i})$\n",
    "\n",
    "\n",
    "Subjected to the constraint    $0\\leq\\alpha_{i}\\leq C$,   $\\sum_{i=1}^{N}\\alpha_{i}y_{i} = 0$\n",
    "\n",
    "Given the relations,\n",
    "\n",
    "$ C = \\alpha_{i} + r_{i}$\n",
    "and since $\\alpha_{i}\\geq0 $ and $r_{i}\\geq0$, we have $0\\leq\\alpha_{i}\\leq C$\n",
    "\n",
    "\n",
    "The objective function of the dual problem is identical to the linear seperable problem. Solving this problem we get optimal decision plane w and b with the margin given as,\n",
    "\n",
    "$(\\sum_{i=1}^{sv}\\sum_{j=1}^{sv}\\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}^{T}.x_{j}))^{\\frac{-1}{2}}$\n",
    "\n",
    "where sv is the set of support vectors - data points that lie along the margin, that is, the data points closest to the hyperplane. \n",
    "\n",
    "From the equation,\n",
    "$y_{i}.(w^{T}.x_{i})\\geq 1 - \\xi_{i}$ and $\\xi_{i}\\geq0$ for i = 1,..,N\n",
    "\n",
    "the support vectors are those $x_i$s where the corresponding $\\xi_{i}$>0 In other words, they are the data points that are either misclassified, or close to the boundary.\n",
    "\n",
    "\n",
    "Also, using the relation, \n",
    "\n",
    "$w = \\sum_{i=1}^{N}y_{i}\\alpha_{i}x_{i}$ \n",
    "\n",
    "and substituting into the equation,\n",
    "\n",
    "$  w^{T}x + b = (\\sum_{i=1}^{N}\\alpha_{i}y_{i}x_{i})^{T}.x +b $\n",
    "\n",
    "$= \\sum_{i=1}^{N}\\alpha_{i}y_{i}<x_{i}.x> + b$\n",
    "\n",
    "Hence, if we’ve found the $\\alpha_{i}s$, in order to make a prediction, we have to calculate a quantity that depends only on the inner product between x and the support vectors.\n",
    "\n",
    "The advantage of maximizing the margin is to make it easier to separate data points that belong to the two different classes. \n",
    "\n",
    "The benefits of solving the dual problem over the primal problem are:\n",
    "\n",
    "1) It lends itself easily to the Kernel Trick. The optimization problem can be written as:\n",
    "\n",
    "$max_{\\alpha} = \\sum_{i=1}^{N}\\alpha_{i} - \\frac{1}{2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}y_{i}y_{j}\\alpha_{i}\\alpha_{j}K(x_{i}x_{j})$\n",
    "\n",
    "such that $\\forall \\alpha_{i} \\geq 0$ and $\\sum_{i=1}^{N}y_{i}\\alpha_{i} = 0$\n",
    "\n",
    "\n",
    "This is almost the same as the original dual formulation, except we compute the kernel function instead of ordinary dot product. This is not possible in the primal formulation where it would be necessary the mapping for each data point.\n",
    "\n",
    "2) Classifying a new data point is much easier in dual as the number of parameters is only equal to the number of data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "1) https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html?highlight=svc#sklearn.svm.LinearSVC\n",
    "\n",
    "\n",
    "2) http://fourier.eng.hmc.edu/e161/lectures/svm/node7.html\n",
    "\n",
    "\n",
    "3) http://cs229.stanford.edu/notes/cs229-notes3.pdf\n",
    "\n",
    "\n",
    "4) https://people.eecs.berkeley.edu/~jordan/courses/281B-spring04/lectures/lec6.pdf\n",
    "\n",
    "\n",
    "5) https://www.quora.com/Why-is-solving-in-the-dual-easier-than-solving-in-the-primal-What-advantages-do-we-get-from-solving-in-the-dual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
